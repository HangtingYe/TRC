seed = 0
n_clusters = 20
eplison = 0.1
max_iter = 200
weight_ = 0.1

[data]
normalization = "quantile"
path = "data/abalone"
y_policy = "mean_std"

[model]
activation = "relu"
d = 256
d_hidden_factor = 1.0
hidden_dropout = 0.1
n_layers = 6
normalization = "batchnorm"
residual_dropout = 0.0
d_embedding = 128

[training]
batch_size = 128
eval_batch_size = 8192
lr = 0.0001
n_epochs = 1000000000
optimizer = "adamw"
patience = 10
weight_decay = 1e-5
